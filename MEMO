TODO list
^^^^^^^^^

For writeup:
------------
TODO: Reimplement forced lengths
TODO: Try, fix reg. prefetch (DG example) / CSEs
  ILP and reg. prefetch interact!
TODO: Functions
TODO: ILP arrays
FIXME: support non-reductive dimensions (what did I mean here?)
FIXME: write names should be assigned during scheduling
FIXME: screwy lower bounds in ILP
FIXME: Leading syncthreads elimination

TODO: Divisibility
TODO: Try, fix indirect addressing

TODO: Implement GT200 matmul, Fermi matmul, DG
TODO: DMA engine threads?
TODO: Deal with equalities that crop up.
TODO: Better user feedback.

Later:
------
TODO: Try different kernels
TODO:   - Tricky: Convolution, Stencil
TODO: Separate all-bulk from non-bulk kernels. (maybe?) (#ifdef?)
TODO: implement efficient ceil_div? (as opposed to floor_div)
TODO: why are corner cases inefficient?
TODO: Use gists (why do disjoint sets arise?)
TODO: variable shuffle detection

Things to consider
^^^^^^^^^^^^^^^^^^

- Depedencies are pointwise for shared loop dimensions
  and global over non-shared ones (between dependent and ancestor)

- multiple insns could fight over which iname gets local axis 0
  -> complicated optimization problem

- Every loop in loopy is opened at most once.
  Too restrictive?

- Loop bounds currently may not depend on parallel dimensions
  Does it make sense to relax this?

- Why do CSEs necessarily have to duplicate the inames?
  -> because that would be necessary for a sequential prefetch

- Cannot do slab decomposition on inames that share a tag with
  other inames
  -> Is that reasonable?

- Parallel dimension splitting/merging via tags
  -> unnecessary?

TODO
^^^^

- implemented_domain may end up being smaller than requested in cse
  evaluations--check that!

- FIXME: Deal with insns losing a seq iname dep in a CSE realization

  a <- cse(reduce(stuff))

- reimplement add_prefetch

- user interface for dim length prescription

- How to determine which variables need to be duplicated for ILP?
  -> Reduction
  -> CSEs?

- Sharing of checks across ILP instances

- Slab decomposition for ILP

- Some things involving CSEs might be impossible to schedule
  a[i,j] = cse(b[i]) * cse(c[j])

- Flag, exploit idempotence

- How should we implement the dim shuffling for odd-size prefetches?

- Better for loop bound generation
  -> Try a triangular loop

Dealt with
^^^^^^^^^^

- Implement condition hoisting
  (needed, e.g., by slab decomposition)

- Check for non-use of hardware axes

- Slab decomposition for parallel dimensions
  - implement at the outermost nesting level regardless
  - bound *all* tagged inames
  - can't slab inames that share tags with other inames (for now)

- Make syntax for iname dependencies

- make syntax for insn dependencies

- Implement get_problems()

- CSE iname duplication might be unnecessary?
  (don't think so: It might be desired to do a full fetch before a mxm k loop
  even if that requires going iterative.)

- Reduction needs to know a neutral element

- Types of reduction variables?

- Generalize reduction to be over multiple variables

Should a dependency on an iname be forced in a CSE?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Local var:

l  | n
g  | y
dl | Err
d  | Err

Private var:

l  | y
g  | y
dl | Err
d  | Err

dg: Invalid-> error

d: is duplicate
l: is tagged as local idx
g: is tagged as group idx

Raise error if dl is targeting a private variable, regardless of whether it's
a dependency or not.

How to represent the schedule
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Focus everything on instructions
  - Each instruction can have its own interpretation of global/local ids.
- Loop variables/splits and such are and remain global
- What about grouped dimensions?
- UniqueTag is the wrong idea! (not really--it's ok per-insn)

Scheduling:
- Find insns whose dependencies are satisfied
- Find maximally shareable loop
- Open that one
- For that opened loop, check if an available insn can run
  - If not, open another loop
  - Else, schedule that instruction
